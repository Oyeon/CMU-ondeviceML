{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbf25a8-1a55-4146-aa8a-32d2cf383e81",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26e90a2-9efa-4d16-b4bb-61fb19f18b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_TRAIN_PATH = '../datasets/lab1_dataset/mnist_train.csv'\n",
    "DATASET_TEST_PATH = '../datasets/lab1_dataset/mnist_test.csv'\n",
    "\n",
    "INPUT_DIMS= 28 * 28\n",
    "HIDDEN_FEATURE_DIMS = 1024\n",
    "OUTPUT_CLASSES = 10\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 10\n",
    "LERANING_RATE = 0.001\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2585e-08c0-44b7-a22d-5f24b9b610c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e37fb4-da0e-4cbc-b753-956d7e293dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        # Load the CSV file\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Assuming the first column contains labels and the rest are pixel values\n",
    "        self.labels = self.data.iloc[:, 0].values  # First column is the label\n",
    "        self.images = self.data.iloc[:, 1:].values.astype('float32')  # Remaining columns are pixel values\n",
    "\n",
    "        # Normalize the pixel values (subtract the mean and divide by std dev)\n",
    "        # Calculate mean and std for normalization\n",
    "        self.mean = np.mean(self.images)\n",
    "        self.std_dev = np.std(self.images)\n",
    "\n",
    "        # Normalize using the calculated mean and std\n",
    "        self.images = (self.images - self.mean) / self.std_dev\n",
    "\n",
    "        # Reshape the images to 28x28\n",
    "        self.images = self.images.reshape(-1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            # Apply transformations (if any)\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define the transformations (you can modify them as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Additional normalization to center data around 0\n",
    "])\n",
    "\n",
    "# Initialize the custom dataset using the CSV file path\n",
    "mnist_dataset = CustomMNISTDataset(DATASET_TRAIN_PATH, transform=transform)\n",
    "val_mnist_dataset = CustomMNISTDataset(DATASET_TEST_PATH, transform=transform)\n",
    "\n",
    "# Use DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_mnist_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Example: Iterating through the dataset\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch images shape: {images.shape}')\n",
    "    print(f'Batch labels shape: {labels.shape}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaca9fe-f7d8-4b10-9f5d-c787feeb2c8c",
   "metadata": {},
   "source": [
    "## Feed-Forward-Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893b636-0506-43ea-9d7f-4025684a95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_DIMS, hidden_size=HIDDEN_FEATURE_DIMS, output_size=OUTPUT_CLASSES):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        \n",
    "        # Note: The input and output layers are not considered hidden layers.\n",
    "        # Input layer to first hidden layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        # First hidden layer to second hidden layer\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        # First hidden layer to second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)        \n",
    "        # Second hidden layer to output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input (batch_size, 1, 28, 28) -> (batch_size, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        # Apply ReLU activation and pass through hidden layers\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Output layer (no activation as it will be used with CrossEntropyLoss)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = GarmentClassifier()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ef6e1-763f-4213-9645-d24d7c26da0f",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842578f2-812e-42d7-a0f2-6da396e3ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LERANING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d78b3-f021-48c6-99c3-a19992548218",
   "metadata": {},
   "source": [
    "## Forward & Backword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ad89b-6736-4dcf-9fca-aafb405a02df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extract inputs (images) and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the gradients for each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute model predictions for the batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss between predictions and actual labels\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backpropagate the loss and compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log the loss every 1000 batches\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # Calculate average loss over the last 1000 batches\n",
    "            print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            \n",
    "            # TensorBoard logging\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "\n",
    "            # Reset the running loss for the next 1000 batches\n",
    "            running_loss = 0.\n",
    "\n",
    "    # Return the average loss of the last set of 1000 batches\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d22a3-1d8c-4d4d-9c88-6340a59ffa70",
   "metadata": {},
   "source": [
    "## Training & Evaluation Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0649fc-f905-4434-8ca9-1cae6914b00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup TensorBoard SummaryWriter\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/fashion_trainer_{timestamp}')\n",
    "\n",
    "# Constants\n",
    "best_vloss = float('inf')  # Initialize to a very large number\n",
    "epoch_number = 0\n",
    "\n",
    "# Variables for measuring training and inference time\n",
    "total_training_time = 0.0\n",
    "total_inference_time = 0.0\n",
    "\n",
    "# Lists to store epoch-wise times\n",
    "training_times = []\n",
    "inference_times_without_warmup = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'=== EPOCH {epoch + 1} ===')\n",
    "\n",
    "    # ---- Training Phase ----\n",
    "    model.train(True)  # Set the model to training mode\n",
    "    start_train_time = time.time()  # Record start time\n",
    "\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Train for one epoch\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # Get predicted class\n",
    "        running_corrects += torch.sum(preds == labels).item()  # Accumulate correct predictions\n",
    "        total_samples += labels.size(0)  # Accumulate total samples\n",
    "\n",
    "    training_accuracy = running_corrects / total_samples  # Calculate accuracy\n",
    "\n",
    "    end_train_time = time.time()  # Record end time\n",
    "    epoch_training_time = end_train_time - start_train_time\n",
    "    total_training_time += epoch_training_time  # Accumulate total training time\n",
    "    training_times.append(epoch_training_time)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Training Time: {epoch_training_time:.4f} seconds')\n",
    "    print(f'Epoch {epoch + 1} Training Accuracy: {training_accuracy:.4f}')\n",
    "\n",
    "    # ---- Validation (Inference) Phase ----\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_vloss = 0.0\n",
    "    inference_times = []\n",
    "\n",
    "    running_vcorrects = 0\n",
    "    total_vsamples = 0\n",
    "\n",
    "    # No gradient computation during inference\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            start_inference_time = time.time()  # Start inference timer\n",
    "\n",
    "            # Forward pass through the model\n",
    "            voutputs = model(vinputs)\n",
    "            _, vpreds = torch.max(voutputs, 1)  # Get predicted class\n",
    "            running_vcorrects += torch.sum(vpreds == vlabels).item()  # Accumulate correct predictions\n",
    "            total_vsamples += vlabels.size(0)  # Accumulate total validation samples\n",
    "\n",
    "            end_inference_time = time.time()  # End inference timer\n",
    "            inference_times.append(end_inference_time - start_inference_time)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    validation_accuracy = running_vcorrects / total_vsamples\n",
    "\n",
    "    # Remove warm-up time (first inference time) for accurate latency measurement\n",
    "    warmup_removed_times = inference_times[1:]\n",
    "    if warmup_removed_times:  # Ensure there's data after warmup removal\n",
    "        avg_inference_time = sum(warmup_removed_times) / len(warmup_removed_times)\n",
    "        total_inference_time += avg_inference_time * len(warmup_removed_times)\n",
    "        inference_times_without_warmup.append(avg_inference_time)\n",
    "        print(f'Epoch {epoch + 1} Avg Inference Time (without warmup): {avg_inference_time:.6f} seconds')\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS - Train: {avg_loss:.4f}, Validation: {avg_vloss:.4f}')\n",
    "    print(f'Epoch {epoch + 1} Validation Accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "    # ---- Logging to TensorBoard ----\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                       epoch + 1)\n",
    "    \n",
    "    writer.add_scalars('Accuracy',\n",
    "                       {'Training': training_accuracy, 'Validation': validation_accuracy},\n",
    "                       epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # ---- Model Checkpoint ----\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'model_{timestamp}_epoch_{epoch + 1}.pth'\n",
    "        # torch.save(model.state_dict(), model_path)\n",
    "        # print(f\"Model saved at {model_path} with validation loss {avg_vloss:.4f}\")\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "# Final stats after training\n",
    "print(f'Total Training Time: {total_training_time:.4f} seconds')\n",
    "print(f'Total Inference Time (excluding warm-up): {total_inference_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25963fd-70ef-4fbf-9b05-d67e5778987b",
   "metadata": {},
   "source": [
    "## Combining Everything without Width & CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3cfee8-0f7e-41ee-92a3-db51ccfd4955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from thop import profile  # For FLOPs calculation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Create a folder to save the plots\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "plot_dir = f'plots_{timestamp}'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 3\n",
    "depths = [1, 2, 3]  # Example depths for the model\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Lists to store results\n",
    "flops_list = []\n",
    "param_counts = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "latency_list = []\n",
    "\n",
    "# Function to get model FLOPs and parameter count\n",
    "def get_model_metrics(model, input_size=(1, 28*28)):\n",
    "    input_tensor = torch.randn(1, *input_size).to(device)  # Ensure the tensor is on the CPU\n",
    "    flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    return flops, params\n",
    "\n",
    "# Loop through different depths and train/evaluate the model\n",
    "for depth in depths:\n",
    "    print(f'=== Training Model with Depth: {depth} ===')\n",
    "\n",
    "    # Define the model with varying depth\n",
    "    class CustomGarmentClassifier(nn.Module):\n",
    "        def __init__(self, depth):\n",
    "            super(CustomGarmentClassifier, self).__init__()\n",
    "            self.input_layer = nn.Linear(28 * 28, HIDDEN_FEATURE_DIMS)\n",
    "            self.hidden_layers = nn.ModuleList([nn.Linear(HIDDEN_FEATURE_DIMS, HIDDEN_FEATURE_DIMS) for _ in range(depth)])\n",
    "            self.output_layer = nn.Linear(HIDDEN_FEATURE_DIMS, OUTPUT_CLASSES)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = F.relu(self.input_layer(x))\n",
    "            for layer in self.hidden_layers:\n",
    "                x = F.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "    # Instantiate and move model to the CPU\n",
    "    model = CustomGarmentClassifier(depth).to(device)\n",
    "\n",
    "    # Get FLOPs and parameter count\n",
    "    flops, params = get_model_metrics(model)\n",
    "    flops_list.append(flops)\n",
    "    param_counts.append(params)\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Measure latency and accuracy\n",
    "    train_acc, val_acc, latency = 0, 0, 0\n",
    "    running_corrects_train = 0\n",
    "    total_samples_train = 0\n",
    "    total_latency = 0\n",
    "\n",
    "    # ---- Training and Validation ----\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train(True)\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        # Train for one epoch (simplified for this example)\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Ensure data is on the CPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects_train += torch.sum(preds == labels).item()\n",
    "            total_samples_train += labels.size(0)\n",
    "\n",
    "        train_acc = running_corrects_train / total_samples_train\n",
    "\n",
    "        end_train_time = time.time()\n",
    "        epoch_training_time = end_train_time - start_train_time\n",
    "        total_latency += epoch_training_time\n",
    "\n",
    "        # Validation (Inference)\n",
    "        model.eval()\n",
    "        running_corrects_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for vinputs, vlabels in val_loader:\n",
    "                vinputs, vlabels = vinputs.to(device), vlabels.to(device)  # Ensure validation data is on the CPU\n",
    "                voutputs = model(vinputs)\n",
    "                _, vpreds = torch.max(voutputs, 1)\n",
    "                running_corrects_val += torch.sum(vpreds == vlabels).item()\n",
    "                total_samples_val += vlabels.size(0)\n",
    "\n",
    "        val_acc = running_corrects_val / total_samples_val\n",
    "\n",
    "    latency_list.append(total_latency / EPOCHS)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    print(f'Depth: {depth}, FLOPs: {flops}, Params: {params}, Train Acc: {train_acc}, Val Acc: {val_acc}, Latency: {total_latency / EPOCHS}')\n",
    "\n",
    "# ---- Plotting Results ----\n",
    "# 1. FLOPs vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(flops_list, val_acc_list, marker='o')\n",
    "plt.title('FLOPs vs Accuracy')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(plot_dir, 'flops_vs_accuracy.png'))\n",
    "\n",
    "# 2. FLOPs vs Latency\n",
    "plt.figure()\n",
    "plt.plot(flops_list, latency_list, marker='o')\n",
    "plt.title('FLOPs vs Latency')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.savefig(os.path.join(plot_dir, 'flops_vs_latency.png'))\n",
    "\n",
    "# 3. Latency vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(latency_list, val_acc_list, marker='o')\n",
    "plt.title('Latency vs Accuracy')\n",
    "plt.xlabel('Latency (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(plot_dir, 'latency_vs_accuracy.png'))\n",
    "\n",
    "print(f'Plots saved to {plot_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd774e-3a13-458b-bf9e-7c046911d63a",
   "metadata": {},
   "source": [
    "# Combining Everything without Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01de519-f990-4dc6-be62-c82c9817189f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from thop import profile  # For FLOPs calculation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a folder to save the plots\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "plot_dir = f'plots_{timestamp}'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 3\n",
    "depths = [1, 2, 3]  # Example depths for the model\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Lists to store results\n",
    "flops_list = []\n",
    "param_counts = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "latency_list = []\n",
    "\n",
    "# Function to get model FLOPs and parameter count\n",
    "def get_model_metrics(model, input_size=(1, 28*28)):\n",
    "    input_tensor = torch.randn(1, *input_size)\n",
    "    flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    return flops, params\n",
    "\n",
    "# Loop through different depths and train/evaluate the model\n",
    "for depth in depths:\n",
    "    print(f'=== Training Model with Depth: {depth} ===')\n",
    "\n",
    "    # Define the model with varying depth\n",
    "    class CustomGarmentClassifier(nn.Module):\n",
    "        def __init__(self, depth):\n",
    "            super(CustomGarmentClassifier, self).__init__()\n",
    "            self.input_layer = nn.Linear(28 * 28, HIDDEN_FEATURE_DIMS)\n",
    "            self.hidden_layers = nn.ModuleList([nn.Linear(HIDDEN_FEATURE_DIMS, HIDDEN_FEATURE_DIMS) for _ in range(depth)])\n",
    "            self.output_layer = nn.Linear(HIDDEN_FEATURE_DIMS, OUTPUT_CLASSES)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = F.relu(self.input_layer(x))\n",
    "            for layer in self.hidden_layers:\n",
    "                x = F.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "    model = CustomGarmentClassifier(depth)\n",
    "\n",
    "    # Get FLOPs and parameter count\n",
    "    flops, params = get_model_metrics(model)\n",
    "    flops_list.append(flops)\n",
    "    param_counts.append(params)\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Measure latency and accuracy\n",
    "    train_acc, val_acc, latency = 0, 0, 0\n",
    "    running_corrects_train = 0\n",
    "    total_samples_train = 0\n",
    "    total_latency = 0\n",
    "\n",
    "    # ---- Training and Validation ----\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train(True)\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        # Train for one epoch (simplified for this example)\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects_train += torch.sum(preds == labels).item()\n",
    "            total_samples_train += labels.size(0)\n",
    "\n",
    "        train_acc = running_corrects_train / total_samples_train\n",
    "\n",
    "        end_train_time = time.time()\n",
    "        epoch_training_time = end_train_time - start_train_time\n",
    "        total_latency += epoch_training_time\n",
    "\n",
    "        # Validation (Inference)\n",
    "        model.eval()\n",
    "        running_corrects_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for vinputs, vlabels in val_loader:\n",
    "                voutputs = model(vinputs)\n",
    "                _, vpreds = torch.max(voutputs, 1)\n",
    "                running_corrects_val += torch.sum(vpreds == vlabels).item()\n",
    "                total_samples_val += vlabels.size(0)\n",
    "\n",
    "        val_acc = running_corrects_val / total_samples_val\n",
    "\n",
    "    latency_list.append(total_latency / EPOCHS)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    print(f'Depth: {depth}, FLOPs: {flops}, Params: {params}, Train Acc: {train_acc}, Val Acc: {val_acc}, Latency: {total_latency / EPOCHS}')\n",
    "\n",
    "# ---- Plotting Results ----\n",
    "# 1. FLOPs vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(flops_list, val_acc_list, marker='o')\n",
    "plt.title('FLOPs vs Accuracy')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(plot_dir, 'flops_vs_accuracy.png'))\n",
    "\n",
    "# 2. FLOPs vs Latency\n",
    "plt.figure()\n",
    "plt.plot(flops_list, latency_list, marker='o')\n",
    "plt.title('FLOPs vs Latency')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.savefig(os.path.join(plot_dir, 'flops_vs_latency.png'))\n",
    "\n",
    "# 3. Latency vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(latency_list, val_acc_list, marker='o')\n",
    "plt.title('Latency vs Accuracy')\n",
    "plt.xlabel('Latency (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(plot_dir, 'latency_vs_accuracy.png'))\n",
    "\n",
    "print(f'Plots saved to {plot_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e11447-becc-4a25-9c57-40fbde0fb708",
   "metadata": {},
   "source": [
    "## Combining Everything with Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094763b-ddb6-429f-9841-853cfadc9ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from thop import profile  # For FLOPs calculation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to create directories based on section input\n",
    "def create_result_folders(section):\n",
    "    base_dir = f'result/section{section}'\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    result_dirs = []\n",
    "    for i in range(1, 4):\n",
    "        result_dir = f'{base_dir}_result{i}'\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        result_dirs.append(result_dir)\n",
    "    \n",
    "    return result_dirs\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 3\n",
    "INPUT_DIMS = 28 * 28\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define different widths and depths\n",
    "widths = [512, 1024, 2048]  # Example widths (narrower and wider than input size)\n",
    "depths = [1, 2, 3]  # Example depths\n",
    "\n",
    "# Lists to store results\n",
    "flops_list = []\n",
    "param_counts = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "train_latency_list = []\n",
    "inference_latency_list = []\n",
    "single_batch_latency_list = []  # List for single-batch inference latencies\n",
    "single_batch_flops_list = []  # List for single-batch FLOPs\n",
    "\n",
    "# Function to get model FLOPs and parameter count\n",
    "def get_model_metrics(model, input_size=(1, INPUT_DIMS)):\n",
    "    input_tensor = torch.randn(1, *input_size)\n",
    "    flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    return flops, params\n",
    "\n",
    "# Ask user for section number\n",
    "section_number = input(\"Enter section number: \")\n",
    "\n",
    "# Choose mode: depth or width variation\n",
    "mode = input(\"Choose mode (depth/width): \").strip().lower()\n",
    "\n",
    "# Create result folders based on the section number\n",
    "result_dirs = create_result_folders(section_number)\n",
    "\n",
    "# Loop through different depths or widths based on the selected mode\n",
    "if mode == \"depth\":\n",
    "    variation = depths\n",
    "    print(f'Varying model depth: {depths}')\n",
    "elif mode == \"width\":\n",
    "    variation = widths\n",
    "    print(f'Varying model width: {widths}')\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode selected. Choose either 'depth' or 'width'.\")\n",
    "\n",
    "for var in variation:\n",
    "    print(f'=== Training Model with {mode.capitalize()}: {var} ===')\n",
    "\n",
    "    # Define the model based on depth or width\n",
    "    class CustomGarmentClassifier(nn.Module):\n",
    "        def __init__(self, var, mode):\n",
    "            super(CustomGarmentClassifier, self).__init__()\n",
    "            self.input_layer = nn.Linear(INPUT_DIMS, var if mode == 'width' else INPUT_DIMS)\n",
    "            \n",
    "            if mode == 'depth':\n",
    "                # Vary depth: dynamically create hidden layers based on depth\n",
    "                self.hidden_layers = nn.ModuleList([nn.Linear(INPUT_DIMS, INPUT_DIMS) for _ in range(var)])\n",
    "                self.output_layer = nn.Linear(INPUT_DIMS, 10)\n",
    "            else:\n",
    "                # Vary width: keep the same number of hidden layers, but change the width\n",
    "                self.hidden_layer = nn.Linear(var, var)\n",
    "                self.output_layer = nn.Linear(var, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, INPUT_DIMS)\n",
    "            x = F.relu(self.input_layer(x))\n",
    "\n",
    "            if mode == 'depth':\n",
    "                # Apply multiple hidden layers for depth variation\n",
    "                for layer in self.hidden_layers:\n",
    "                    x = F.relu(layer(x))\n",
    "            else:\n",
    "                # Apply a single hidden layer for width variation\n",
    "                x = F.relu(self.hidden_layer(x))\n",
    "\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "    # Create the model\n",
    "    model = CustomGarmentClassifier(var, mode)\n",
    "\n",
    "    # Get FLOPs and parameter count\n",
    "    flops, params = get_model_metrics(model)\n",
    "    flops_list.append(flops)\n",
    "    param_counts.append(params)\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Measure latency and accuracy\n",
    "    train_acc, val_acc, train_latency, inference_latency = 0, 0, 0, 0\n",
    "    running_corrects_train = 0\n",
    "    total_samples_train = 0\n",
    "    total_train_latency = 0\n",
    "    total_inference_latency = 0\n",
    "\n",
    "    # ---- Training and Validation ----\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train(True)\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        # Train for one epoch (simplified for this example)\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects_train += torch.sum(preds == labels).item()\n",
    "            total_samples_train += labels.size(0)\n",
    "\n",
    "        train_acc = running_corrects_train / total_samples_train\n",
    "\n",
    "        end_train_time = time.time()\n",
    "        epoch_training_time = end_train_time - start_train_time\n",
    "        total_train_latency += epoch_training_time\n",
    "\n",
    "        # Validation (Inference) - Measure Inference Latency\n",
    "        model.eval()\n",
    "        running_corrects_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for vinputs, vlabels in val_loader:\n",
    "                start_inference_time = time.time()  # Start timing inference\n",
    "\n",
    "                voutputs = model(vinputs)\n",
    "                _, vpreds = torch.max(voutputs, 1)\n",
    "                running_corrects_val += torch.sum(vpreds == vlabels).item()\n",
    "                total_samples_val += vlabels.size(0)\n",
    "\n",
    "                end_inference_time = time.time()  # End timing inference\n",
    "                total_inference_latency += (end_inference_time - start_inference_time)\n",
    "\n",
    "        val_acc = running_corrects_val / total_samples_val\n",
    "\n",
    "    train_latency_list.append(total_train_latency / EPOCHS)  # Average training latency per epoch\n",
    "    inference_latency_list.append(total_inference_latency / len(val_loader))  # Average inference latency per batch\n",
    "    val_acc_list.append(val_acc)\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    print(f'{mode.capitalize()}: {var}, FLOPs: {flops}, Params: {params}, Train Acc: {train_acc}, Val Acc: {val_acc}, Train Latency: {total_train_latency / EPOCHS}, Inference Latency: {total_inference_latency / len(val_loader)}')\n",
    "\n",
    "    # ---- Single-Batch Inference ----\n",
    "    # Perform single-batch inference and measure FLOPs and latency\n",
    "    model.eval()\n",
    "    single_batch = next(iter(val_loader))\n",
    "    inputs, labels = single_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_single_batch_time = time.time()  # Start timing single-batch inference\n",
    "\n",
    "        # Forward pass for single batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        end_single_batch_time = time.time()  # End timing\n",
    "        single_batch_latency = end_single_batch_time - start_single_batch_time\n",
    "\n",
    "        # Calculate FLOPs for single-batch inference\n",
    "        single_batch_flops, _ = profile(model, inputs=(inputs,), verbose=False)\n",
    "\n",
    "    # Store latency and FLOPs\n",
    "    single_batch_latency_list.append(single_batch_latency)\n",
    "    single_batch_flops_list.append(single_batch_flops)\n",
    "\n",
    "    print(f'Single-Batch Inference for {mode.capitalize()} {var}: FLOPs: {single_batch_flops}, Latency: {single_batch_latency:.6f} seconds')\n",
    "\n",
    "# ---- Plotting Results ----\n",
    "# 1. FLOPs vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(flops_list, val_acc_list, marker='o')\n",
    "plt.title('FLOPs vs Accuracy')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(result_dirs[0], 'flops_vs_accuracy.png'))\n",
    "\n",
    "# 2. FLOPs vs Latency (Single-Batch Inference)\n",
    "plt.figure()\n",
    "plt.plot(single_batch_flops_list, single_batch_latency_list, marker='o')\n",
    "plt.title('FLOPs vs Single-Batch Inference Latency')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Single-Batch Inference Latency (s)')\n",
    "plt.savefig(os.path.join(result_dirs[1], 'flops_vs_latency.png'))\n",
    "\n",
    "# 3. Latency vs Accuracy (Single-Batch Inference)\n",
    "plt.figure()\n",
    "plt.plot(single_batch_latency_list, val_acc_list, marker='o')\n",
    "plt.title('Single-Batch Inference Latency vs Accuracy')\n",
    "plt.xlabel('Single-Batch Inference Latency (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(result_dirs[2], 'latency_vs_accuracy.png'))\n",
    "\n",
    "print(f'Plots saved in {result_dirs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9581bd-9621-45d1-bd31-48802bb76dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter section number:  s4\n",
    "# Choose mode (depth/width):  width\n",
    "# Varying model width: [512, 1024, 2048]\n",
    "# === Training Model with Width: 512 ===\n",
    "# Width: 512, FLOPs: 668672.0, Params: 669706.0, Train Acc: 0.9579881886920337, Val Acc: 0.9677967796779678, Train Latency: 10.520967165629068, Inference Latency: 0.0012251343727111817\n",
    "# Single-Batch Inference for Width 512: FLOPs: 6686720.0, Latency: 0.000338 seconds\n",
    "# === Training Model with Width: 1024 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8683c2c-565d-42af-a6a9-9b566c097fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Experiment Setup:\n",
    "# Depth Variation: We varied the depth of the model across 1, 2, and 3 hidden layers to observe how the model's complexity affects performance.\n",
    "# Metrics Measured: For each model configuration, we tracked FLOPs (Floating Point Operations), latency (training time per epoch), and accuracy (both training and validation).\n",
    "# Efficiency Metrics: FLOPs and latency are used to measure the computational cost, while accuracy reflects model performance.\n",
    "# 2. Key Tradeoffs:\n",
    "# FLOPs vs Accuracy: A higher number of FLOPs indicates more computational complexity, but this doesn’t always correlate with better accuracy. A balance needs to be struck where the increase in complexity improves accuracy without diminishing returns.\n",
    "# FLOPs vs Latency: Models with higher FLOPs generally take longer to train. However, hardware and optimization strategies can influence this relationship.\n",
    "# Latency vs Accuracy: This relationship is crucial when optimizing models for real-time applications. A model that takes too long to compute may not be practical even if it achieves high accuracy.\n",
    "# 3. Results and Trends:\n",
    "# Accuracy vs Depth: As the depth of the model increases, the accuracy typically improves. However, this increase may taper off beyond a certain depth, where the marginal gains in accuracy are minimal compared to the computational cost.\n",
    "# FLOPs Impact: Models with more FLOPs tend to be more accurate, but the increase in computational cost might not justify the small improvement in accuracy for larger models.\n",
    "# Latency Impact: Depth and width variations both affect latency. Larger models take more time to train, and the relationship between latency and accuracy highlights the tradeoff between efficiency and performance.\n",
    "# 4. Best Tradeoff:\n",
    "# The best trade-off is often found with a moderate depth (e.g., depth 2), where the accuracy is reasonably high, but the computational cost (in terms of both FLOPs and latency) is not prohibitively expensive. For some applications, slight reductions in accuracy may be acceptable in exchange for significantly lower computational costs.\n",
    "# Conclusion:\n",
    "# The experiments and visualizations provide insights into how varying model depth affects the tradeoffs between accuracy and efficiency. The generated plots illustrate how FLOPs and latency scale with accuracy, giving a clear picture of the efficiency-performance tradeoff in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9960b-df14-4cae-8c4c-df1c45b2365f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553421bf-03c5-4013-a75c-a58176d61779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41dda250-1d81-4a6e-bd74-13d191155c51",
   "metadata": {},
   "source": [
    "## Data Loader & Training code integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b7b38-bd64-436c-9697-0d9d7e76423b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from thop import profile  # For FLOPs calculation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define CustomMNISTDataset\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.labels = self.data.iloc[:, 0].values  # First column is the label\n",
    "        self.images = self.data.iloc[:, 1:].values.astype('float32')  # Remaining columns are pixel values\n",
    "\n",
    "        # Normalize using the calculated mean and std\n",
    "        self.mean = np.mean(self.images)\n",
    "        self.std_dev = np.std(self.images)\n",
    "        self.images = (self.images - self.mean) / self.std_dev\n",
    "\n",
    "        # Reshape the images to 28x28 initially\n",
    "        self.images = self.images.reshape(-1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert the NumPy array to a PIL image\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # Convert normalized image back to 0-255 for PIL\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations for resizing, cropping, and no transformation\n",
    "resize_transform_14 = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),  # Resize to 14x14\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "resize_transform_20 = transforms.Compose([\n",
    "    transforms.Resize((20, 20)),  # Resize to 20x20\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Ensure image is 28x28 before cropping\n",
    "    transforms.CenterCrop(20),    # Center crop to 20x20\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "no_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),        # No resizing or cropping, just convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Prompt user to choose a transformation\n",
    "print(\"Choose a transformation:\")\n",
    "print(\"1: Resize to 14x14\")\n",
    "print(\"2: Resize to 20x20\")\n",
    "print(\"3: Center Crop to 20x20\")\n",
    "print(\"4: No Transformation (28x28)\")\n",
    "\n",
    "transformation_choice = input(\"Enter the number of the transformation (1, 2, 3, or 4): \").strip()\n",
    "\n",
    "# Dynamically set INPUT_DIMS based on transformation\n",
    "if transformation_choice == \"1\":\n",
    "    transform = resize_transform_14\n",
    "    INPUT_DIMS = 14 * 14  # For 14x14 input, INPUT_DIMS is 196\n",
    "elif transformation_choice == \"2\":\n",
    "    transform = resize_transform_20\n",
    "    INPUT_DIMS = 20 * 20  # For 20x20 input, INPUT_DIMS is 400\n",
    "elif transformation_choice == \"3\":\n",
    "    transform = crop_transform\n",
    "    INPUT_DIMS = 20 * 20  # For 20x20 crop, INPUT_DIMS is 400\n",
    "elif transformation_choice == \"4\":\n",
    "    transform = no_transform\n",
    "    INPUT_DIMS = 28 * 28  # For 28x28 input, INPUT_DIMS is 784\n",
    "\n",
    "# Define constants\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 10\n",
    "\n",
    "# Load datasets with selected transformation\n",
    "mnist_dataset = CustomMNISTDataset(DATASET_TRAIN_PATH, transform=transform)\n",
    "val_mnist_dataset = CustomMNISTDataset(DATASET_TEST_PATH, transform=transform)\n",
    "\n",
    "# Use DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_mnist_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Function to create directories based on section input\n",
    "def create_result_folders(section):\n",
    "    base_dir = f'result/section{section}'\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    result_dirs = []\n",
    "    for i in range(1, 4):\n",
    "        result_dir = f'{base_dir}_result{i}'\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        result_dirs.append(result_dir)\n",
    "    \n",
    "    return result_dirs\n",
    "\n",
    "# Constants for the training\n",
    "EPOCHS = 2\n",
    "learning_rate = 0.001\n",
    "widths = [1024]  # Example widths (narrower and wider than input size)\n",
    "depths = [1024]  # Example depths\n",
    "DEPTH_LAYER = 2\n",
    "\n",
    "# Function to get model FLOPs and parameter count\n",
    "def get_model_metrics(model, input_size=(1, INPUT_DIMS)):\n",
    "    input_tensor = torch.randn(1, *input_size)\n",
    "    flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    return flops, params\n",
    "\n",
    "# Ask user for section number\n",
    "section_number = input(\"Enter section number: \")\n",
    "\n",
    "# Choose mode: depth or width variation\n",
    "mode = input(\"Choose mode (depth/width): \").strip().lower()\n",
    "\n",
    "# Create result folders based on the section number\n",
    "result_dirs = create_result_folders(section_number)\n",
    "\n",
    "# Lists to store results\n",
    "flops_list = []\n",
    "param_counts = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "train_latency_list = []\n",
    "inference_latency_list = []\n",
    "single_batch_latency_list = []\n",
    "single_batch_flops_list = []\n",
    "\n",
    "# Loop through different depths or widths based on the selected mode\n",
    "if mode == \"depth\":\n",
    "    variation = depths\n",
    "    depth_layer = DEPTH_LAYER\n",
    "elif mode == \"width\":\n",
    "    variation = widths\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode selected. Choose either 'depth' or 'width'.\")\n",
    "\n",
    "for var in variation:\n",
    "    print(f'=== Training Model with {mode.capitalize()}: {var} ===')\n",
    "\n",
    "    # Define the model based on depth or width\n",
    "    class CustomGarmentClassifier(nn.Module):\n",
    "        def __init__(self, var, mode):\n",
    "            super(CustomGarmentClassifier, self).__init__()\n",
    "            self.input_layer = nn.Linear(INPUT_DIMS, var if mode == 'width' else var)\n",
    "            \n",
    "            if mode == 'depth':\n",
    "                # Vary depth: dynamically create hidden layers based on depth\n",
    "                self.hidden_layers = nn.ModuleList([nn.Linear(var, var) for _ in range(depth_layer)])\n",
    "                self.output_layer = nn.Linear(var, 10)\n",
    "            else:\n",
    "                # Vary width: keep the same number of hidden layers, but change the width\n",
    "                self.hidden_layer = nn.Linear(var, var)\n",
    "                self.output_layer = nn.Linear(var, 10)\n",
    "                \n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, INPUT_DIMS)\n",
    "            x = F.relu(self.input_layer(x))\n",
    "\n",
    "            if mode == 'depth':\n",
    "                # Apply multiple hidden layers for depth variation\n",
    "                for layer in self.hidden_layers:\n",
    "                    x = F.relu(layer(x))\n",
    "            else:\n",
    "                # Apply a single hidden layer for width variation\n",
    "                x = F.relu(self.hidden_layer(x))\n",
    "\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "    # Create the model\n",
    "    model = CustomGarmentClassifier(var, mode)\n",
    "\n",
    "    # Get FLOPs and parameter count\n",
    "    flops, params = get_model_metrics(model)\n",
    "    flops_list.append(flops)\n",
    "    param_counts.append(params)\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Measure latency and accuracy\n",
    "    running_corrects_train = 0\n",
    "    total_samples_train = 0\n",
    "    total_train_latency = 0\n",
    "    total_inference_latency = 0\n",
    "\n",
    "    # ---- Training and Validation ----\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train(True)\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        # Train for one epoch\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects_train += torch.sum(preds == labels).item()\n",
    "            total_samples_train += labels.size(0)\n",
    "\n",
    "        train_acc = running_corrects_train / total_samples_train\n",
    "        total_train_latency += time.time() - start_train_time\n",
    "\n",
    "        # Validation (Inference)\n",
    "        model.eval()\n",
    "        running_corrects_val = 0\n",
    "        total_samples_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for vinputs, vlabels in val_loader:\n",
    "                start_inference_time = time.time()\n",
    "                voutputs = model(vinputs)\n",
    "                _, vpreds = torch.max(voutputs, 1)\n",
    "                running_corrects_val += torch.sum(vpreds == vlabels).item()\n",
    "                total_samples_val += vlabels.size(0)\n",
    "                total_inference_latency += time.time() - start_inference_time\n",
    "\n",
    "        val_acc = running_corrects_val / total_samples_val\n",
    "\n",
    "    train_latency_list.append(total_train_latency / EPOCHS)  # Average training latency per epoch\n",
    "    inference_latency_list.append(total_inference_latency / len(val_loader))  # Average inference latency per batch\n",
    "    val_acc_list.append(val_acc)\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    print(f'{mode.capitalize()}: {var}, FLOPs: {flops}, Params: {params}, Train Acc: {train_acc}, Val Acc: {val_acc}')\n",
    "\n",
    "    # ---- Single-Batch Inference ----\n",
    "    # Perform single-batch inference and measure FLOPs and latency\n",
    "    model.eval()\n",
    "    single_batch = next(iter(val_loader))\n",
    "    inputs, labels = single_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_single_batch_time = time.time()\n",
    "        outputs = model(inputs)\n",
    "        single_batch_latency = time.time() - start_single_batch_time\n",
    "\n",
    "        # Calculate FLOPs for single-batch inference\n",
    "        single_batch_flops, _ = profile(model, inputs=(inputs,), verbose=False)\n",
    "\n",
    "    single_batch_latency_list.append(single_batch_latency)\n",
    "    single_batch_flops_list.append(single_batch_flops)\n",
    "\n",
    "    print(f'Single-Batch Inference for {mode.capitalize()} {var}: FLOPs: {single_batch_flops}, Latency: {single_batch_latency:.6f} seconds')\n",
    "\n",
    "# ---- Plotting Results ----\n",
    "# 1. FLOPs vs Accuracy\n",
    "plt.figure()\n",
    "plt.plot(flops_list, val_acc_list, marker='o')\n",
    "plt.title('FLOPs vs Accuracy')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(result_dirs[0], 'flops_vs_accuracy.png'))\n",
    "\n",
    "# 2. FLOPs vs Latency (Single-Batch Inference)\n",
    "plt.figure()\n",
    "plt.plot(single_batch_flops_list, single_batch_latency_list, marker='o')\n",
    "plt.title('FLOPs vs Single-Batch Inference Latency')\n",
    "plt.xlabel('FLOPs')\n",
    "plt.ylabel('Single-Batch Inference Latency (s)')\n",
    "plt.savefig(os.path.join(result_dirs[1], 'flops_vs_latency.png'))\n",
    "\n",
    "# 3. Latency vs Accuracy (Single-Batch Inference)\n",
    "plt.figure()\n",
    "plt.plot(single_batch_latency_list, val_acc_list, marker='o')\n",
    "plt.title('Single-Batch Inference Latency vs Accuracy')\n",
    "plt.xlabel('Single-Batch Inference Latency (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(os.path.join(result_dirs[2], 'latency_vs_accuracy.png'))\n",
    "\n",
    "print(f'Plots saved in {result_dirs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70595b-340b-4187-9c92-f66ea3c17ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1cce2e2-3de3-462c-96dd-eabf0b0fcdca",
   "metadata": {},
   "source": [
    "## # of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0369a8-9ea8-46eb-a612-e2715e008add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to count trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage for GarmentClassifier model\n",
    "model = GarmentClassifier()\n",
    "total_params = count_parameters(model)\n",
    "print(f'Total trainable parameters: {total_params}')\n",
    "\n",
    "\n",
    "# 1. 파라미터 수 계산 함수 (Trainable Parameters Count)\n",
    "# 모델의 파라미터 수는 각 계층의 파라미터 개수를 계산하여 더하는 방식으로 구할 수 있습니다. 각 Linear 계층에서의 파라미터 개수는 입력 뉴런 수 × 출력 뉴런 수 + 출력 뉴런 수(바이어스)로 계산됩니다.\n",
    "\n",
    "# 설명:\n",
    "# model.parameters()를 통해 모델의 모든 파라미터에 접근합니다.\n",
    "# 각 파라미터의 numel() 함수를 사용해 그 파라미터 텐서의 요소 개수를 얻고, 이를 모두 더합니다.\n",
    "# requires_grad=True인 파라미터만을 대상으로 하여 학습 가능한 파라미터의 수를 계산합니다.\n",
    "# 수식으로 파라미터 계산 (Closed Form):\n",
    "# fc1: 입력 뉴런 784 (28x28) × 은닉층 뉴런 1024 + 바이어스(1024) = 784 * 1024 + 1024 = 803840\n",
    "# fc2: 은닉층 뉴런 1024 × 은닉층 뉴런 1024 + 바이어스(1024) = 1024 * 1024 + 1024 = 1049600\n",
    "# fc3: 은닉층 뉴런 1024 × 출력층 뉴런 10 + 바이어스(10) = 1024 * 10 + 10 = 10250\n",
    "# 총 파라미터 수: 803840 + 1049600 + 10250 = 1863690"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060c19-e93d-4b4e-8949-4c45f8f6e0dd",
   "metadata": {},
   "source": [
    "## FLOPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e973733-06d0-4b63-b7c6-d27b40aace8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute FLOPs for GarmentClassifier\n",
    "def compute_flops(model, input_size=(1, 28*28)):\n",
    "    flops = 0\n",
    "    \n",
    "    # fc1 FLOPs: (input_size * hidden_size) + hidden_size\n",
    "    flops += (input_size[1] * model.fc1.out_features) + model.fc1.out_features\n",
    "    \n",
    "    # fc2 FLOPs: (hidden_size * hidden_size) + hidden_size\n",
    "    flops += (model.fc1.out_features * model.fc2.out_features) + model.fc2.out_features\n",
    "    \n",
    "    # fc3 FLOPs: (hidden_size * output_size) + output_size\n",
    "    flops += (model.fc2.out_features * model.fc3.out_features) + model.fc3.out_features\n",
    "    \n",
    "    return flops\n",
    "\n",
    "# Example usage for GarmentClassifier model\n",
    "flops = compute_flops(model)\n",
    "print(f'Total FLOPs for one forward pass: {flops}')\n",
    "\n",
    "# 2. FLOPs 계산 함수 (Floating Point Operations)\n",
    "\n",
    "# FLOPs는 신경망에서의 부동소수점 연산 횟수를 나타내며, 각 계층에서 발생하는 곱셈과 덧셈 연산의 개수를 고려합니다. 여기서는 모델의 각 Linear 계층에 대한 연산을 계산합니다. Linear 계층에서는 다음 연산이 수행됩니다:\n",
    "\n",
    "# 곱셈 연산: 입력 뉴런 수 × 출력 뉴런 수\n",
    "# 덧셈 연산: 출력 뉴런 수 (바이어스 덧셈)\n",
    "\n",
    "# 설명:\n",
    "# 각 Linear 계층에서의 곱셈 연산과 덧셈 연산을 더해 FLOPs를 계산합니다.\n",
    "# 예를 들어, fc1 계층에서는 28*28 크기의 입력이 1024개의 은닉층 뉴런으로 전달되므로, 곱셈 연산은 784 * 1024개, 덧셈 연산은 1024개가 발생합니다.\n",
    "# 이 계산을 모든 계층에 대해 수행하여 총 FLOPs를 구합니다.\n",
    "# FLOPs 계산 방식:\n",
    "# fc1: (784 × 1024) + 1024 = 803840 FLOPs\n",
    "# fc2: (1024 × 1024) + 1024 = 1049600 FLOPs\n",
    "# fc3: (1024 × 10) + 10 = 10250 FLOPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f65fd2-c219-4a75-8988-4116ab39ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
